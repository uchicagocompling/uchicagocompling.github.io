<!DOCTYPE html>
<html>
<head>
<title>UChicago CompLing Lab</title>
<meta charset="UTF-8">
<link rel="stylesheet" href="style2.css">
</head>

<h1>UChicago CompLing Lab</h1>

    <ul id="navbar">
          <li id="home"><a href="index.html" title="Home">Home</a></li>
          <li id="people"><a href="people.html" title="People">People</a></li>
          <li id="projects"><a href="projects.html" title="Projects">Projects</a></li>

    </ul>

<h2>Projects</h2>

<div class="project">

  Members of our lab are engaged in a range of projects aimed at better understanding the linguistic competence and nature of sensitivities
  of pre-trained language models, studying and improving competence with compositional meaning understanding in NLP models, as well as
  using computational models to test hypotheses about the mechanisms underlying language processing in humans. <br><br>

  <hr>

</div>

<h3 class="project">Controlled evaluation of compositional meaning understanding in pre-trained LMs</h3>
  <div class="project">

    In this body of work our priority is to establish reliable and robust evaluations for the compositional meaning
    competence of pre-trained language models. This work employs controlled tests that tease apart compositional meaning
    understanding from more superficial predictive behaviors, and often draws on methodology from psycholinguistics and cognitive neuroscience. <br><br>

    <b>Example papers</b>
    <ul>
    <li> Misra, K., Rayz, J., Ettinger, A. COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models. <a href="https://arxiv.org/pdf/2210.01963.pdf">[PDF]</a></li>
    <li> Pandia, L., Ettinger, A. (2021). Sorting through the noise: Testing robustness of information processing in pre-trained language models. <i>Proceedings of The 2021 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://arxiv.org/pdf/2109.12393.pdf">[PDF]</a></li>
    <li> Yu, L., Ettinger, A. (2021). On the Interplay Between Fine-tuning and Composition in Transformers. <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Findings</i>. <a href="https://arxiv.org/pdf/2105.14668.pdf">[PDF]</a></li>
    <li> Yu, L., Ettinger, A. (2020). Assessing Phrasal Representation and Composition in Transformers. <i>Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf">[PDF]</a></li>
    <li> Ettinger, A. (2020). What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. <i>Transactions of the Association for Computational Linguistics</i>. <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00298">[PDF]</a> <a href="https://github.com/aetting/lm-diagnostics">[Diagnostic tests and code]</a></li>
    <li> Ettinger, A., Elgohary, A., Phillips, C., Resnik, P. (2018). Assessing Composition in Sentence Vector Representations. <i>Proceedings of the 27th International Conference on Computational Linguistics</i>. <a href="https://arxiv.org/pdf/1809.03992.pdf">[PDF]</a></li>
    <li> Ettinger, A., Elgohary, A., Resnik, P. (2016). Probing for semantic evidence of composition by means of simple classification tasks. <i>Proceedings of the First Workshop on Evaluating Vector Space Representations for NLP, ACL 2016</i>. <a href="https://aclanthology.org/W16-2524.pdf">[PDF]</a></li>
  </ul>

  </div>

<div style="margin-left:40px;">

  ~~~~~~~~~~~~~~~~~

</div>

<h3 class="project">Assessment of other linguistic competence and sensitivity in pre-trained LMs</h3>
  <div class="project">

    In this line of work we apply tests from psycholinguistics to examine pragmatic sensitivities developed (or not developed)
    by pre-trained language models as a result of their pre-training. <br><br>

    <b>Example papers</b><br><br>

    <i>Property knowledge and inductive reasoning</i>
    <ul>
    <li> Misra, K., Rayz, J., Ettinger, A. COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models. <a href="https://arxiv.org/pdf/2210.01963.pdf">[PDF]</a></li>
    <li> Misra, K., Taylor Rayz, J., Ettinger, A. (2022). A Property Induction Framework for Neural Language Models. <i>Proceedings of the 44th Annual Meeting of the Cognitive Science Society</i>. <a href="https://arxiv.org/pdf/2205.06910.pdf">[PDF]</a></li>
    <li> Misra, K., Ettinger, A., Taylor Rayz, J. (2021). Do language models learn typicality judgments from text?. <i>Proceedings of the 43rd Annual Meeting of the Cognitive Science Society</i>. <a href="https://arxiv.org/pdf/2105.02987.pdf">[PDF]</a></li>
    </ul>

    <i>Pragmatic competence</i>
    <ul>
    <li> Kim, S.J., Yu, L., Ettinger, A. (2022). “No, they did not”: Dialogue response dynamics in pre-trained language models. <i>Proceedings of the 29th International Conference on Computational Linguistics (COLING)</i>. <a href="https://arxiv.org/pdf/2210.02526.pdf">[PDF]</a></li>
    <li> Pandia, L., Cong, Y., Ettinger, A. (2021). Pragmatic competence of pre-trained language models through the lens of discourse connectives. <i>Proceedings of the 2021 SIGNLL Conference on Computational Natural Language Learning (CoNLL)</i>. <a href="https://arxiv.org/pdf/2109.12951.pdf">[PDF]</a></li>
    </ul>

    <i>Assorted dimensions of model linguistic sensitivity and representation</i>
    <ul>
    <li> Wu, Q., Ettinger, A. (2021). Variation and generality in encoding of syntactic anomaly information in sentence embeddings. <i>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</i>. <a href="https://arxiv.org/pdf/2111.06644.pdf">[PDF]</a></li>
    <li> Misra, K., Ettinger, A., Taylor Rayz, J. (2020). Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming. <i>Findings of ACL: EMNLP 2020</i>. <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415.pdf">[PDF]</a></li>
    <li> Klafka, J., Ettinger, A. (2020). Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. <a href="https://www.aclweb.org/anthology/2020.acl-main.434.pdf">[PDF]</a> <a href="https://github.com/jklafka/context-probes">[Probing datasets and code]</a></li>
  </ul>
  </div>

  <div style="margin-left:40px;">

    ~~~~~~~~~~~~~~~~~

  </div>

<h3 class="project">Computational modeling of real-time language processing in humans</h3>
    <div class="project">

      In this body of work we design computational models to test hypotheses about the cognitive mechanisms driving
      the robust and rapid language processing in the human brain. We draw selectively on probabilistic and representational
      measures from large language models to increase stimulus-level statistical sensitivities of our cognitive models.<br><br>

      <b>Example papers</b>
      <ul>
      <li> Li, J., Ettinger, A. Heuristic Interpretation as Rational Inference: A Computational Model of the N400 and P600 in Language Processing. <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4115173">[PDF]</a></li>
      <li> Ettinger, A., Feldman, N.H., Resnik, P., Phillips, C. (2016). Modeling N400 amplitude using vector space models of word representation. <i>Proceedings of the 38th Annual Conference of the Cognitive Science Society.</i> <a href="https://cogsci.mindmodeling.org/2016/papers/0256/paper0256.pdf">[PDF]</a></li>
      <li> Ettinger, A., Linzen, T. (2016). Evaluating vector space models using human semantic priming results. <i>Proceedings of the First Workshop on Evaluating Vector Space Representations for NLP, ACL 2016</i>. <a href="https://aclanthology.org/W16-2513.pdf">[PDF]</a></li>
      </ul>
    </div>

    <div style="margin-left:40px;">

      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    </div>

<!--
<h3>Publications</h3>
    <p>Pandia, L., Ettinger, A. (2021). Sorting through the noise: Testing robustness of information processing in pre-trained language models. <i>Proceedings of The 2021 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://arxiv.org/pdf/2109.12393.pdf">[PDF]</a></p>

    <p>Pandia, L., Cong, Y., Ettinger, A. (2021). Pragmatic competence of pre-trained language models through the lens of discourse connectives. <i>Proceedings of the 2021 SIGNLL Conference on Computational Natural Language Learning</i>. <a href="https://arxiv.org/pdf/2109.12951.pdf">[PDF]</a></p>

    <p>Wu, Q., Ettinger, A. (2021). Variation and generality in encoding of syntactic anomaly information in sentence embeddings. <i>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</i>. <a href="pdfs/WuEttinger2021.pdf">[PDF]</a></p>

    <p>Yu, L., Ettinger, A. (2021). On the Interplay Between Fine-tuning and Composition in Transformers. <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Findings</i>. <a href="https://arxiv.org/pdf/2105.14668.pdf">[PDF]</a></p>

    <p>Misra, K., Ettinger, A., Taylor Rayz, J. (2021). Do language models learn typicality judgments from text?. <i>Proceedings of the 43rd Annual Meeting of the Cognitive Science Society</i>. <a href="https://arxiv.org/pdf/2105.02987.pdf">[PDF]</a></p>

    <p>Yu, L., Ettinger, A. (2020). Assessing Phrasal Representation and Composition in Transformers. <i>Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf">[PDF]</a></p>

    <p>Misra, K., Ettinger, A., Taylor Rayz, J. (2020). Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming. <i>Findings of ACL: EMNLP 2020</i>. <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415.pdf">[PDF]</a></p>

    <p>Toshniwal, S., Wiseman, S., Ettinger, A., Gimpel, K., Livescu, K. (2020). Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks. <i>Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.685.pdf">[PDF]</a></p>

    <p>Klafka, J., Ettinger, A. (2020). Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. <a href="https://www.aclweb.org/anthology/2020.acl-main.434.pdf">[PDF]</a> <a href="https://github.com/jklafka/context-probes">[Probing datasets and code]</a></p>

    <p>Toshniwal, S., Ettinger, A., Gimpel, K., Livescu, K. (2020). PeTra: A Sparsely Supervised Memory Model for People Tracking. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. <a href="https://www.aclweb.org/anthology/2020.acl-main.481.pdf">[PDF]</a> <a href="https://colab.research.google.com/drive/17xT1QKCbj_tOFpiszHxuLkhjXLPp_hkd?usp=sharing">[Colab notebook]</a></p>

    <p>Ettinger, A. (2020). What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. <i>Transactions of the Association for Computational Linguistics</i>. <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00298">[PDF]</a> <a href="https://github.com/aetting/lm-diagnostics">[Diagnostic tests and code]</a></p>

-->

</html>
