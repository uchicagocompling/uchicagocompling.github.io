<!DOCTYPE html>
<html>
<head>
<title>UChicago CompLing Lab</title>
<meta charset="UTF-8">
<link rel="stylesheet" href="style2.css">
</head>

<h1>UChicago CompLing Lab</h1>

    <ul id="navbar">
          <li id="home"><a href="index.html" title="Home">Home</a></li>
          <li id="people"><a href="people.html" title="People">People</a></li>
          <li id="projects"><a href="projects.html" title="Projects">Projects</a></li>

    </ul>

<h2>Projects</h2>

<div class="project">

  Members of our lab are engaged in a range of projects aimed at better understanding the linguistic competence and nature of sensitivities
  of pre-trained language models, studying and improving competence with compositional meaning understanding in NLP models, as well as
  using computational models to test hypotheses about the mechanisms underlying language processing in humans. <br><br>

  <hr style="width:800px;">

</div>

<h3>Controlled evaluation of compositional meaning understanding in pre-trained LMs</h3>
  <div class="project">

    In this body of work our priority is to establish reliable and robust evaluations for the compositional meaning
    competence of pre-trained language models. This work employs controlled tests that tease apart compositional meaning
    understanding from more superficial predictive behaviors, and often draws on methodology from psycholinguistics and cognitive neuroscience. <br><br>

    <b>Example papers:</b>
    <ul>
    <li> Misra, K., Rayz, J., Ettinger, A. (2022). <a href="https://arxiv.org/pdf/2210.01963.pdf">COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models.</a></li>
    <li> Pandia, L., Ettinger, A. (2021). <a href="https://arxiv.org/pdf/2109.12393.pdf">Sorting through the noise: Testing robustness of information processing in pre-trained language models.</a> EMNLP 2021.</li>
    <li> Yu, L., Ettinger, A. (2021). <a href="https://arxiv.org/pdf/2105.14668.pdf">On the Interplay Between Fine-tuning and Composition in Transformers.</a> ACL Findings 2021.</li>
    <li> Yu, L., Ettinger, A. (2020). <a href="https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf">Assessing Phrasal Representation and Composition in Transformers.</a> EMNLP 2020.</li>
    <li> Ettinger, A. (2020). <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00298">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models.</a> TACL 2020.</li>
    <li> Ettinger, A., Elgohary, A., Phillips, C., Resnik, P. (2018). <a href="https://arxiv.org/pdf/1809.03992.pdf">Assessing Composition in Sentence Vector Representations.</a> COLING 2018.</li>
    <li> Ettinger, A., Elgohary, A., Resnik, P. (2016). <a href="https://aclanthology.org/W16-2524.pdf">Probing for semantic evidence of composition by means of simple classification tasks.</a> RepEval 2016.</li>
  </ul>

  </div>

<div style="margin-left:40px;">

  ~~~~~~~~~~~~~~~~~

</div>

<h3>Assessment of other linguistic competence and sensitivity in pre-trained LMs</h3>
  <div class="project">

    In this line of work we apply tests from psycholinguistics to examine pragmatic sensitivities developed (or not developed)
    by pre-trained language models as a result of their pre-training. <br><br>

    <b>Example papers:</b><br><br>

    <i>Property knowledge and inductive reasoning</i>
    <ul>
    <li> Misra, K., Rayz, J., Ettinger, A. (2022). <a href="https://arxiv.org/pdf/2210.01963.pdf">COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models.</a></li>
    <li> Misra, K., Taylor Rayz, J., Ettinger, A. (2022). <a href="https://arxiv.org/pdf/2205.06910.pdf">A Property Induction Framework for Neural Language Models.</a> CogSci 2022.</li>
    <li> Misra, K., Ettinger, A., Taylor Rayz, J. (2021). <a href="https://arxiv.org/pdf/2105.02987.pdf">Do language models learn typicality judgments from text?</a> CogSci 2021.</li>
    </ul>

    <i>Pragmatic competence</i>
    <ul>
    <li> Kim, S.J., Yu, L., Ettinger, A. (2022). <a href="https://arxiv.org/pdf/2210.02526.pdf">“No, they did not”: Dialogue response dynamics in pre-trained language models.</a> COLING 2021.</li>
    <li> Pandia, L., Cong, Y., Ettinger, A. (2021). <a href="https://arxiv.org/pdf/2109.12951.pdf">Pragmatic competence of pre-trained language models through the lens of discourse connectives.</a> CoNLL 2021.</li>
    </ul>

    <i>Assorted dimensions of model linguistic sensitivity and representation</i>
    <ul>
    <li> Wu, Q., Ettinger, A. (2021). <a href="https://arxiv.org/pdf/2111.06644.pdf">Variation and generality in encoding of syntactic anomaly information in sentence embeddings.</a> BlackboxNLP 2021.</li>
    <li> Misra, K., Ettinger, A., Taylor Rayz, J. (2020). <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415.pdf">Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming.</a> EMNLP Findings 2020.</li>
    <li> Klafka, J., Ettinger, A. (2020). <a href="https://www.aclweb.org/anthology/2020.acl-main.434.pdf">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words.</a> ACL 2020.</li>
  </ul>
  </div>

  <div style="margin-left:40px;">

    ~~~~~~~~~~~~~~~~~

  </div>

<h3>Computational modeling of real-time language processing in humans</h3>
    <div class="project">

      In this body of work we design computational models to test hypotheses about the cognitive mechanisms driving
      the robust and rapid language processing in the human brain. We draw selectively on probabilistic and representational
      measures from large language models to increase stimulus-level statistical sensitivities of our cognitive models.<br><br>

      <b>Example papers:</b>
      <ul>
      <li> Li, J., Ettinger, A. <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4115173">Heuristic Interpretation as Rational Inference: A Computational Model of the N400 and P600 in Language Processing.</a></li>
      <li> Ettinger, A., Feldman, N.H., Resnik, P., Phillips, C. (2016). <a href="https://cogsci.mindmodeling.org/2016/papers/0256/paper0256.pdf">Modeling N400 amplitude using vector space models of word representation.</a> CogSci 2016.</li>
      <li> Ettinger, A., Linzen, T. (2016). <a href="https://aclanthology.org/W16-2513.pdf">Evaluating vector space models using human semantic priming results.</a> RepEval 2016.</li>
      </ul>
    </div>

    <div style="margin-left:40px;">

      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    </div>

<!--
<h3>Publications</h3>
    <p>Pandia, L., Ettinger, A. (2021). Sorting through the noise: Testing robustness of information processing in pre-trained language models. <i>Proceedings of The 2021 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://arxiv.org/pdf/2109.12393.pdf">[PDF]</a></p>

    <p>Pandia, L., Cong, Y., Ettinger, A. (2021). Pragmatic competence of pre-trained language models through the lens of discourse connectives. <i>Proceedings of the 2021 SIGNLL Conference on Computational Natural Language Learning</i>. <a href="https://arxiv.org/pdf/2109.12951.pdf">[PDF]</a></p>

    <p>Wu, Q., Ettinger, A. (2021). Variation and generality in encoding of syntactic anomaly information in sentence embeddings. <i>Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</i>. <a href="pdfs/WuEttinger2021.pdf">[PDF]</a></p>

    <p>Yu, L., Ettinger, A. (2021). On the Interplay Between Fine-tuning and Composition in Transformers. <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Findings</i>. <a href="https://arxiv.org/pdf/2105.14668.pdf">[PDF]</a></p>

    <p>Misra, K., Ettinger, A., Taylor Rayz, J. (2021). Do language models learn typicality judgments from text?. <i>Proceedings of the 43rd Annual Meeting of the Cognitive Science Society</i>. <a href="https://arxiv.org/pdf/2105.02987.pdf">[PDF]</a></p>

    <p>Yu, L., Ettinger, A. (2020). Assessing Phrasal Representation and Composition in Transformers. <i>Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf">[PDF]</a></p>

    <p>Misra, K., Ettinger, A., Taylor Rayz, J. (2020). Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming. <i>Findings of ACL: EMNLP 2020</i>. <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415.pdf">[PDF]</a></p>

    <p>Toshniwal, S., Wiseman, S., Ettinger, A., Gimpel, K., Livescu, K. (2020). Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks. <i>Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing</i>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.685.pdf">[PDF]</a></p>

    <p>Klafka, J., Ettinger, A. (2020). Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. <a href="https://www.aclweb.org/anthology/2020.acl-main.434.pdf">[PDF]</a> <a href="https://github.com/jklafka/context-probes">[Probing datasets and code]</a></p>

    <p>Toshniwal, S., Ettinger, A., Gimpel, K., Livescu, K. (2020). PeTra: A Sparsely Supervised Memory Model for People Tracking. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. <a href="https://www.aclweb.org/anthology/2020.acl-main.481.pdf">[PDF]</a> <a href="https://colab.research.google.com/drive/17xT1QKCbj_tOFpiszHxuLkhjXLPp_hkd?usp=sharing">[Colab notebook]</a></p>

    <p>Ettinger, A. (2020). What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. <i>Transactions of the Association for Computational Linguistics</i>. <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00298">[PDF]</a> <a href="https://github.com/aetting/lm-diagnostics">[Diagnostic tests and code]</a></p>

-->

</html>
